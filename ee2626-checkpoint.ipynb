{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "093e6bcbd039d3dce06f32c47e4bfa6c69d30fe1",
    "id": "o_7zdELmhGgn",
    "outputId": "63e5fc1e-53f9-425b-e1b8-fff56f0ac84e"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import bz2\n",
    "from sklearn import model_selection, preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of the most import libraries for natural language processing is NLTK. It provides more than 50 corpora and lexical resources and interfaces to work with them, also it provides text processing libraries including tokenization, stemming, parsing, classification and etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to load a data set to work with.Amazon reviews dataset which contains 360000 text reviews and their tags is used. This dataset contains positive and negative reviews. The task is to classify reviews into either of classes. This task is known as sentiment analysis in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "6e29213d819811e44eea35a8b87af5276c7ce27b",
    "id": "13psF-UxlLn1"
   },
   "outputs": [],
   "source": [
    "def splitReviewsLabels(lines):\n",
    "    reviews = []\n",
    "    labels = []\n",
    "    for review in tqdm(lines):\n",
    "        rev = reviewToX(review)\n",
    "        label = reviewToY(review)\n",
    "        reviews.append(rev[:512])\n",
    "        labels.append(label)\n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now changing to labels.0 for negative,1 for poisitive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "c0ce0e4c678d12ac91deaef433bf1d32b1f87bcf",
    "id": "4nTvOtd7oWJM"
   },
   "outputs": [],
   "source": [
    "def reviewToY(review):\n",
    "    return 0 if review.split(' ')[0] == '__label__1' else 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "9a4aa9856bea537502506833f1df9960f6172f15",
    "id": "z2DijnI-plux"
   },
   "outputs": [],
   "source": [
    "def reviewToX(review):\n",
    "    review = review.split(' ', 1)[1][:-1].lower()\n",
    "    review = re.sub('\\d','0',review)\n",
    "    if 'www.' in review or 'http:' in review or 'https:' in review or '.com' in review:\n",
    "        review = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", review)\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print confusion matrix\n",
    "def conf_mat(preds, y):\n",
    "    TN=0\n",
    "    FP=0\n",
    "    FN=0\n",
    "    TP=0\n",
    "    for x in range(len(preds)):\n",
    "        if preds[x] == 0 and y[x] == 0:\n",
    "            TN+=1\n",
    "        elif preds[x] == 1 and y[x] == 1:\n",
    "            TP+=1\n",
    "        elif preds[x] == 0 and y[x] == 1:\n",
    "            FN+=1\n",
    "        elif preds[x] == 1 and y[x] == 0:\n",
    "            FP+=1\n",
    "    print(\"TN: \", TN, \" | FP: \", FP)\n",
    "    print(\"FN: \", FN, \" | TP: \", TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "c596ea6c292833146271bdf36238fd8f527a1f06"
   },
   "outputs": [],
   "source": [
    "train_file = bz2.BZ2File('./input/train.ft.txt.bz2')\n",
    "test_file = bz2.BZ2File('./input/test.ft.txt.bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "5db1d078bb74a49c4f94cd12feb48ce52dd087a2"
   },
   "outputs": [],
   "source": [
    "train_lines = train_file.readlines()\n",
    "test_lines = test_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "5af875b476a33943c6e5c7b7bc3986394e3a1fe3"
   },
   "outputs": [],
   "source": [
    "train_lines = [x.decode('utf-8') for x in train_lines[:20000]]\n",
    "test_lines = [x.decode('utf-8') for x in test_lines[:4000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to limited computing capacity,we are truncating the dataset into 20000 training set and 4000 test set i.e. in ration 8:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "766396c805eb0ec0bde8c7e99c3ca6212fcfebc3",
    "id": "5l-8fcOPljvP",
    "outputId": "ab2281fe-798d-4568-ba21-8165b6633cfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 20000/20000 [00:00<00:00, 62887.97it/s]\n",
      "100%|█████████████████████████████████| 4000/4000 [00:00<00:00, 68962.86it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load from the file\n",
    "train_x, train_y = splitReviewsLabels(train_lines)\n",
    "test_x,test_y = splitReviewsLabels(test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos:  10257\n",
      "Neg:  9743\n"
     ]
    }
   ],
   "source": [
    "#find data distribution\n",
    "pos=0\n",
    "neg=0\n",
    "for val in train_y:\n",
    "    if val == 1:\n",
    "        pos+=1\n",
    "    else:\n",
    "        neg+=1\n",
    "print(\"Pos: \", pos)\n",
    "print(\"Neg: \", neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see how are the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buyer beware: this is a self-published book, and if you want to know why--read a few paragraphs! those 0 star reviews must have been written by ms. haddon's family and friends--or perhaps, by herself! i can't imagine anyone reading the whole thing--i spent an evening with the book and a friend and we were in hysterics reading bits and pieces of it to one another. it is most definitely bad enough to be entered into some kind of a \"worst book\" contest. i can't believe amazon even sells this kind of thing. may\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(train_x[6])\n",
    "print(train_y[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode the labels into numerical values since the values of the labels are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "test_y = encoder.fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(train_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to calculate the probability of each class. Doing so we will obtain priors in the Bayes formula. It can be easily done by counting number of occurrences of each class divided by the total number of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48715, 0.51285])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi=np.array([sum(train_y==0)/len(train_y),sum(train_y==1)/len(train_y)])\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that we can not feed text as string to the model, so we need a way to represent text as numerical value to the model. One of the easiest encoding that we can use is the Bag of Words (BoW) encoding. It takes into account words and their frequency of occurrence in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer function from sklearn.feature_extraction.text is the function that encode text using the BoW method. It basically returns the matrix of token counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(train_x)\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x).todense()\n",
    "xtest_count =  count_vect.transform(test_x).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 41278)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to calculate the probability of occurrence of each word per class, i.e., likelihood. To do that we are going to create a dataframe that contain three columns: words,class1,and class2. The class1 column will be containing the likelihood for class1 (negative reviews) and class2 column is going to contain the likelihood for class2 (positive reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, one important technicality must be considered. The thing is that since some words might not occur in one of the classes, then the likelihood of those words will become zero. Having a zero likelihood for a word is very damaging to the model's performance because it causes the posterior to be zero which does not make sense. To elevate this problem a kind of smoothing which is called Laplace smoothing is used which is defined as follows.\n",
    "\n",
    "$$ P(X_i \\mid C_j) = \\frac{count_{ij}+\\alpha}{count_j+|V|+1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above formula $ count_{ij} $ is the number of occurrences of word i in class j. $ count_j $ is total number of words in class j and |v| is the vocab size in class j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordFreq = pd.DataFrame(columns=['words','class1','class2'])\n",
    "wordFreq['words'] = count_vect.get_feature_names()\n",
    "\n",
    "x_train_class1 = xtrain_count[train_y==0]\n",
    "x_train_class2 = xtrain_count[train_y==1]\n",
    "\n",
    "count_class1 = np.sum(x_train_class1,axis=0)\n",
    "count_class2 = np.sum(x_train_class2,axis=0)\n",
    "\n",
    "vocab_size1 = len(np.where(count_class1==0)[1])\n",
    "vocab_size2 = len(np.where(count_class2==0)[1])\n",
    "\n",
    "alpha=10\n",
    "count_class1 = np.array( (count_class1+alpha) /(np.sum(count_class1)+vocab_size1 +1))\n",
    "count_class2 = np.array( (count_class2+alpha) /(np.sum(count_class2)+vocab_size2 +1))\n",
    "\n",
    "wordFreq['class1'] = pd.Series(count_class1.ravel())\n",
    "wordFreq['class2'] = pd.Series(count_class2.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to iterate through all sentences for both train and validation data to calculate the accuracy on both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy is: 0.8814\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.zeros(len(xtrain_count))\n",
    "for i in range(len(xtrain_count)):\n",
    "    idx = np.where(xtrain_count[i,:]!=0)[1]\n",
    "    lh1 = wordFreq['class1'].iloc[idx].prod()\n",
    "    lh2 = wordFreq['class2'].iloc[idx].prod()\n",
    "    posterior1 = lh1*pi[0]\n",
    "    posterior2 = lh2 * pi[1]\n",
    "\n",
    "    if posterior1>posterior2:\n",
    "        train_preds[i] = 0\n",
    "    else:\n",
    "        train_preds[i] = 1\n",
    "\n",
    "\n",
    "matches = np.sum(train_y==train_preds)\n",
    "print('Train accuracy is: '+str(matches/len(train_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "20000\n",
      "TN:  8779  | FP:  964\n",
      "FN:  1408  | TP:  8849\n"
     ]
    }
   ],
   "source": [
    "# make confusion matrix\n",
    "print(len(train_preds))\n",
    "print(len(train_y))\n",
    "conf_mat(train_preds, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy is: 0.8395\n",
      "TN:  1652  | FP:  299\n",
      "FN:  343  | TP:  1706\n"
     ]
    }
   ],
   "source": [
    "test_preds = np.zeros(len(xtest_count))\n",
    "for i in range(len(xtest_count)):\n",
    "    idx = np.where(xtest_count[i,:]!=0)[1]\n",
    "    lh1 = wordFreq['class1'].iloc[idx].prod()\n",
    "    lh2 = wordFreq['class2'].iloc[idx].prod()\n",
    "    posterior1 = lh1*pi[0]\n",
    "    posterior2 = lh2 * pi[1]\n",
    "\n",
    "    if posterior1>posterior2:\n",
    "        test_preds[i] = 0\n",
    "    else:\n",
    "        test_preds[i] = 1\n",
    "\n",
    "    temp = 1\n",
    "\n",
    "matches = np.sum(test_y==test_preds)\n",
    "print('Validation accuracy is: '+str(matches/len(test_preds)))\n",
    "conf_mat(test_preds, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Validation accuracy is 0.8395, which is very promising given the simplicity of the model. Our objective was to learn naive bayes and implement it from scratch without library. we succeded in doing that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However for improving performance there are more sophisticated methods like RNN,LSTM Transformers and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
